{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66af6199-ce1e-4e7f-bf0c-e8cbd18a62b6",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "## What is Bag of Words?\n",
    "\n",
    "The **Bag of Words (BoW)** is a natural language processing (NLP) technique used to represent text data. It simplifies text into a numerical format that machine learning models can understand. The BoW model disregards grammar, word order, or semantics, focusing solely on the frequency of words in the text.\n",
    "\n",
    "### Why is it Called \"Bag of Words\"?\n",
    "\n",
    "The term \"Bag of Words\" emphasizes that:\n",
    "1. It treats text as a collection (or \"bag\") of words.\n",
    "2. The position or sequence of words is ignored.\n",
    "3. Only the count or presence of words matters.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does Bag of Words Work?\n",
    "\n",
    "### 1. Text Preprocessing:\n",
    "Before applying BoW, text data is preprocessed. Common preprocessing steps include:\n",
    "- **Lowercasing**: Convert all text to lowercase to avoid case sensitivity.\n",
    "- **Tokenization**: Split text into individual words (tokens).\n",
    "- **Stop Word Removal**: Remove common words like \"the\", \"is\", \"and\", which may not carry significant meaning.\n",
    "- **Stemming or Lemmatization**: Reduce words to their base forms (e.g., \"running\" â†’ \"run\").\n",
    "\n",
    "### 2. Vocabulary Creation:\n",
    "- Create a unique list of words (vocabulary) from the entire corpus (collection of text data).\n",
    "- Each word in the vocabulary becomes a feature.\n",
    "\n",
    "### 3. Vector Representation:\n",
    "- Each document is represented as a vector of word counts.\n",
    "- The length of the vector is equal to the size of the vocabulary.\n",
    "- Each element of the vector corresponds to the frequency (or presence) of a specific word in the document.\n",
    "\n",
    "---\n",
    "\n",
    "### Example of Bag of Words:\n",
    "\n",
    "#### Input Documents:\n",
    "1. \"I love machine learning.\"\n",
    "2. \"Machine learning is amazing.\"\n",
    "3. \"I love amazing AI.\"\n",
    "\n",
    "#### Preprocessing:\n",
    "- Lowercase the text.\n",
    "- Remove stop words like \"is\" and \"the\".\n",
    "- Tokenize the text.\n",
    "\n",
    "#### Vocabulary:\n",
    "`['ai', 'amazing', 'i', 'learning', 'love', 'machine']`\n",
    "\n",
    "#### Bag of Words Representation:\n",
    "| Document Index | AI | Amazing | I | Learning | Love | Machine |\n",
    "|----------------|----|---------|---|----------|------|---------|\n",
    "| 1              | 0  | 0       | 1 | 1        | 1    | 1       |\n",
    "| 2              | 0  | 1       | 0 | 1        | 0    | 1       |\n",
    "| 3              | 1  | 1       | 1 | 0        | 1    | 0       |\n",
    "\n",
    "Each document is now represented as a numerical vector.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Bag of Words\n",
    "\n",
    "1. **Simple to Understand and Implement**:\n",
    "   - BoW is straightforward and requires minimal computational resources for small datasets.\n",
    "\n",
    "2. **Effective for Small Text Data**:\n",
    "   - Works well for applications like spam detection or sentiment analysis when text data is limited.\n",
    "\n",
    "3. **Compatible with ML Models**:\n",
    "   - BoW vectors can be used as input for algorithms like Naive Bayes, SVM, and Logistic Regression.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of Bag of Words\n",
    "\n",
    "1. **Ignores Word Order**:\n",
    "   - BoW does not capture the sequence or context of words, which can lead to loss of meaning.\n",
    "\n",
    "2. **High Dimensionality**:\n",
    "   - For large vocabularies, the vector size can become very large, leading to memory and computational inefficiencies.\n",
    "\n",
    "3. **Sparse Representation**:\n",
    "   - Many words in the vocabulary may not appear in a given document, resulting in sparse vectors (vectors with many zeros).\n",
    "\n",
    "4. **No Semantic Information**:\n",
    "   - BoW treats synonyms (e.g., \"good\" and \"great\") as completely different words and does not account for polysemy (words with multiple meanings).\n",
    "\n",
    "---\n",
    "\n",
    "## Enhancements Over Bag of Words\n",
    "\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - Weighs words based on their importance in a document relative to the entire corpus.\n",
    "   - Reduces the impact of common but less informative words.\n",
    "\n",
    "2. **Word Embeddings**:\n",
    "   - Techniques like Word2Vec, GloVe, and FastText encode words into dense vectors, capturing semantic relationships and context.\n",
    "\n",
    "3. **N-grams**:\n",
    "   - Instead of single words, BoW can use N-grams (sequences of N consecutive words) to capture some contextual information.\n",
    "\n",
    "---\n",
    "\n",
    "## Bag of Words in Python\n",
    "\n",
    "### Example Implementation with `CountVectorizer` (scikit-learn)\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample Data\n",
    "documents = [\n",
    "    \"I love machine learning.\",\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"I love amazing AI.\"\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and Transform the Data\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Display Vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display Bag of Words Representation\n",
    "print(\"Bag of Words Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a5856c1-f2de-4abc-ac96-44d6120d2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fa1ffc-d924-4ca9-aabf-f2dab8a65781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'text':['People watch youtube people',\n",
    "           'youtube is good',\n",
    "            'people write comment',\n",
    "            'user post vidoes'\n",
    "           ],'output':[1,1,0,0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71cc8725-568d-457a-a2a8-2fb1be9b2a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People watch youtube people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>youtube is good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user post vidoes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text  output\n",
       "0  People watch youtube people       1\n",
       "1              youtube is good       1\n",
       "2         people write comment       0\n",
       "3             user post vidoes       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ee5cef7-2e40-472a-a6d6-7e552535b156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COunt vectorizer is a bag of word technique\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf9e9fbc-db0b-408e-8b21-8ca2fafe2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5da0b29-57dc-4c2e-a599-0349d555afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 3, 'watch': 7, 'youtube': 9, 'is': 2, 'good': 1, 'write': 8, 'comment': 0, 'user': 5, 'post': 4, 'vidoes': 6}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9daebc-6421-4179-b5b0-01efb1ec8630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03baf5-2715-4126-babb-17b68d03d2f1",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "## What is an N-gram?\n",
    "\n",
    "An **N-gram** is a contiguous sequence of `N` items (words, characters, or tokens) from a given text or speech data. N-grams are commonly used in Natural Language Processing (NLP) to capture context and patterns within text data. They can help retain some information about the order and relationship of words, unlike simpler models like Bag of Words.\n",
    "\n",
    "---\n",
    "\n",
    "## Types of N-grams\n",
    "\n",
    "### 1. **Unigrams (1-grams)**:\n",
    "   - Single-word tokens.\n",
    "   - Example: For the sentence **\"I love data science\"**, the unigrams are:\n",
    "     ```\n",
    "     [\"I\", \"love\", \"data\", \"science\"]\n",
    "     ```\n",
    "\n",
    "### 2. **Bigrams (2-grams)**:\n",
    "   - Pairs of consecutive words.\n",
    "   - Example: For the sentence **\"I love data science\"**, the bigrams are:\n",
    "     ```\n",
    "     [\"I love\", \"love data\", \"data science\"]\n",
    "     ```\n",
    "\n",
    "### 3. **Trigrams (3-grams)**:\n",
    "   - Groups of three consecutive words.\n",
    "   - Example: For the sentence **\"I love data science\"**, the trigrams are:\n",
    "     ```\n",
    "     [\"I love data\", \"love data science\"]\n",
    "     ```\n",
    "\n",
    "### 4. **N-grams (N > 3)**:\n",
    "   - Groups of `N` consecutive words.\n",
    "   - Example: For **\"I love data science\"**, the 4-gram (N=4) is:\n",
    "     ```\n",
    "     [\"I love data science\"]\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "## How Does N-grams Work?\n",
    "\n",
    "1. **Tokenization**: \n",
    "   - The text is split into words or tokens.\n",
    "2. **Grouping**:\n",
    "   - Words are grouped into `N` consecutive tokens.\n",
    "\n",
    "For example, given the sentence:  \n",
    "**\"Natural Language Processing is interesting.\"**\n",
    "\n",
    "- Unigrams: `[\"Natural\", \"Language\", \"Processing\", \"is\", \"interesting\"]`\n",
    "- Bigrams: `[\"Natural Language\", \"Language Processing\", \"Processing is\", \"is interesting\"]`\n",
    "- Trigrams: `[\"Natural Language Processing\", \"Language Processing is\", \"Processing is interesting\"]`\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of N-grams\n",
    "\n",
    "1. **Text Representation**:\n",
    "   - N-grams provide a better understanding of context compared to unigrams (single words).\n",
    "\n",
    "2. **Language Modeling**:\n",
    "   - Predict the next word based on preceding N-1 words. For example, in predictive text input or autocomplete.\n",
    "\n",
    "3. **Text Classification**:\n",
    "   - Used to extract features for tasks like sentiment analysis, spam detection, or topic modeling.\n",
    "\n",
    "4. **Speech Recognition**:\n",
    "   - N-grams help identify common word sequences to improve recognition accuracy.\n",
    "\n",
    "5. **Machine Translation**:\n",
    "   - Useful in capturing relationships between words to translate phrases effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of N-grams\n",
    "\n",
    "1. **Captures Context**:\n",
    "   - Bigrams and higher-order N-grams capture the relationship between consecutive words, unlike Bag of Words.\n",
    "\n",
    "2. **Simplicity**:\n",
    "   - Easy to compute and implement.\n",
    "\n",
    "3. **Improved Features**:\n",
    "   - Provides richer features for machine learning models compared to unigrams.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of N-grams\n",
    "\n",
    "1. **High Dimensionality**:\n",
    "   - As `N` increases, the number of possible N-grams grows exponentially, leading to high memory usage and sparsity.\n",
    "\n",
    "2. **Data Dependency**:\n",
    "   - Higher-order N-grams require more data to generate meaningful patterns. For example, trigrams may not appear frequently in small datasets.\n",
    "\n",
    "3. **Loss of Global Context**:\n",
    "   - Even with N-grams, the model may still not capture long-range dependencies in text.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation in Python\n",
    "\n",
    "Hereâ€™s an example of how to generate N-grams using Python:\n",
    "\n",
    "### Example: Generate N-grams\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample Text\n",
    "text = [\"I love natural language processing and machine learning\"]\n",
    "\n",
    "# Initialize CountVectorizer for Bigrams (N=2)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "# Display Vocabulary\n",
    "print(\"Bigrams Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display Bigrams Frequency\n",
    "print(\"Bigrams Matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc323abc-2037-4b67-b7ce-2b83b91a4283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
